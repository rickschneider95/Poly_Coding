---
title: "Housing"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Include libraries:
```{r,message=FALSE,warning =FALSE}
library(tidyverse)
library(ggplot2)
library(readr)
```
#Links and ideas:
VERY HELPFUL LINK:
https://rstudio-pubs-static.s3.amazonaws.com/150743_fbe2be64165349798440e35351653b16.html

https://www.datacamp.com/community/tutorials/linear-regression-R#coefficients
1) look at p-value --> H0:b=0, H1:b!=0. accept H0 if p>a (i.e.) for large p. We want a small p!, b should be different from 0! In simple terms, a p-value indicates whether or not you can reject or accept a hypothesis. The hypothesis, in this case, is that the predictor is not meaningful for your model.
2) look at R2 --> the better the closer to 1. R2=(explained variation of the model)/(Total variation of the model). R2 = 0.99 --> explains 99% of the variability.
In the blue rectangle, notice that there’s two different R², one multiple and one adjusted. The multiple is the R² that you saw previously. One problem with this R² is that it cannot decrease as you add more independent variables to your model, it will continue increasing as you make the model more complex, even if these variables don’t add anything to your predictions (like the example of the number of siblings). For this reason, the adjusted R² is probably better to look at if you are adding more than one variable to the model, since it only increases if it reduces the overall error of the predictions.
3) plot the residuals --> plot(lm_model$residuals). Should be random, no pattern! If pattern -> If you have more data, your simple linear model will not be able to generalize well.


Multicolinearity:
http://blog.minitab.com/blog/understanding-statistics/handling-multicollinearity-in-regression-analysis
Multicollinearity increases the standard errors of the coefficients. Increased standard errors in turn means that coefficients for some independent variables may be found not to be significantly different from 0. In other words, by overinflating the standard errors, multicollinearity makes some variables statistically insignificant when they should be significant. Without multicollinearity (and thus, with lower standard errors), those coefficients might be significant.

idea:
1)visualize one linear regression with 1 variable
2)add them all
3)analyse and remove, come up with new ones (e.g. muliply # of bedrooms with ...)

#Introduction
The goal of this thesis is to study and predict housing prices (response variable) based on predictor variables that provide data on the physical characteristics of properties, such as their size, age, location, etc.


########## WARNING: bedroom bathroom rather continuuous data. Year categorical? ######

#Exploratory data analysis:
Lets have a short look at our data set:
```{r,message=FALSE}
#read data in and have a look at it:
house <- read_csv("data/realestate.csv")
head(house)

#we only work with "house":
attach(house)

#delete column "ID":
house <- house[,-1]

#dim of house:
dim(house)
names(house)

#is "Pool" binary or does t count the number of pools?
house %>% select(Pool) %>% distinct()

#which values does "Quality" take?:
house %>% select(Quality) %>% distinct()

#which values does "AdjHighway" take?:
house %>% select(AdjHighway) %>% distinct()

View(house)
```
We store our data set "realestat.csv" into a variable called house. The first insight is that the column "ID" is not needed, as it only stores the index of the houses. As a result, we can delete this column. The data set now contains 11 columns and 522 rows. The variables are as follows: "Price" (quantitative; the price of the property in question), "Sqft" (quantitative; size), "Bedroom" (quantitative; number of bedrooms), "Bathroom" (quantitative; number of bathrooms), "Airconditioning" (qualitative; yes: 1, no: 0), "Garage" (quantitative; number of garages (or parking spots)), "Pool" (qualitative; yes: 1, no: 0), "YearBuild" (qualitative; year in which the house was built), "Quality" (qualitative; the build-quality of the house on a scale of 1 to 3), "Lot" (quantitative, the surface of the land), and finally "AdjHighway"(qualitative; 1: next to a highway, 0: not close or next to a highway).

#EDA
First, we are going to study the variables themselves and their impact on the price of the house. Next, we are going to generate to simple regression models based on the results of the analysis. Finally, we will look at the relations that might exist amongst te variables in order to remove a few of them from oure model (to improve it by reducing the effects of multicollinearity) and to create new variables that might improve our model.

Lets start by analysing the "Price" variable:
```{r}
#summary statistics:
summary(Price)

#histogram:
ggplot(data=house, aes(Price)) +
  geom_histogram(col="white",
                 bins=35) +
  labs(title="Histogram for Price")

#is it normal?
qqnorm(Price)

#boxplot:
ggplot(data=house, aes(x="", y=Price)) +
  geom_boxplot()+
  labs(title="Boxplot for Price")

#Check for impossible values:
sum(is.na(Price))
sum(Price[Price < 0])
  
```
Looking at the histogram, the price follows approximately a normal distribution. However, it is right skewed, meaning that it has a long right tail. However, the qq-plot shows that the normal assumptions is very weak. The median price of a house is 229900$, while the mean price is 277894$. This is due to the fact that there are a few outliers, reaching a maximum price of 920000$. The boxplot of the price-variable verifies our assumption with the outliers. The data is indeed right skewed with a lot of ouliers. 
Furthermore, there are no missing values and no negative values, which shows that the data seem to be complete and not faulty.

"Sqft":
```{r}
#summary:
summary(Sqft)
#histogram:
ggplot(data=house, aes(Sqft)) +
  geom_histogram(col="white",
                 bins=35) +
  labs(title="Histogram for Sqft")

#null values?
sum(is.na(Sqft))

#scatter plot:
ggplot(data = house, aes(x=Sqft,y=Price))+
  geom_point()+
  labs(title="Scatter plot of the price as a function of the size (Sqft)")
#correlation:
cor(Price,Sqft)
```

"Sqft" is similarly distributed to "Price". The mean size of a house is 2261, the median is 2061. The distribution is right-skewed as well. There are no null values and all values are larger than 0.
Looking at the scatter plot between the "Price" variable and the "Sqft" variable, there seems to be a linear relationship between the 2 variables. The high positive correlation of 0.819 seems to reinforce our initial assumption of a linear relationship between the two variables.


"Bedroom":
```{r}
#barplot:
ggplot(data=house, aes(Bedroom)) +
  geom_bar()

#table of the different values:
table(Bedroom)

#null values?
sum(is.na(Bedroom))

#mean for the different values of the categorical variable:
mean_cat <- house %>% 
              group_by(Bedroom) %>% 
              summarise(m=mean(Price))
#plot the mean values for the diffrent bedrooms:
ggplot(data=mean_cat, aes(x=Bedroom,y=m)) +
  geom_point()+
  labs(title="Plot of the mean values of the price against the number of bedrooms")

```
Most of the houses have 3 bedrooms (202 bedrooms), followed by 4 bedrooms (179), then 2 (64). There are no missing values and the data lies within {0,...,6}.
There seems to be an interesting relationship between the number of bedrooms and the price of the house. We plotted the mean values for the price having a certain amount of bedrooms. We can observe a linear trend. More bedrooms seem to increase the price of the property.


"Bathroom":
```{r}
ggplot(data=house, aes(Bathroom)) +
  geom_bar()+
  labs(title="Barplot of the variable Bathroom")
table(Bathroom)

#null values?
sum(is.na(Bathroom))

#mean for the different values of the categorical variable:
mean_cat <- house %>% 
              group_by(Bathroom) %>% 
              summarise(m=mean(Price))
mean_cat
#plot the mean values for the diffrent bedrooms:
ggplot(data=mean_cat, aes(x=Bathroom,y=m)) +
  geom_point()+
  labs(title="Plot of the mean values of the price against the number of bathrooms")

```
Most houses have 3 bathrooms, followed by houses with 2 and 4 bathrooms. There are no missing values and the data lies within {0,1,...,7}. As a result, there seems to be no invalid data points.
Similar to the bedrooms, the number of bathrooms seems to have an influence n the price. More bathrooms means that the property will be sold a higher price (general tendency).


"Airconditioning":
```{r}
#table + calculate percentage:
ac_table<-table(Airconditioning)
ac_table
100*ac_table[2]/(ac_table[1]+ac_table[2])

#null values?
sum(is.na(Airconditioning))

#mean for the two values:
house %>% 
  group_by(Airconditioning) %>%
  summarise(m=mean(Price))
```
83.12% (i.e. 434) of the houses have airconditioning. There are no missing values. The mean price for houses with airconditioning is higher (i.e. 295801) than for houses without airconditioning (i.e. 189583).


"Garage":
```{r}
#barplot
ggplot(data=house, aes(Garage)) +
  geom_bar()+
  labs(title="Barplot of the variable Garage")

table(Garage)

#null values?
sum(is.na(Garage))

#mean for the different values of the categorical variable:
mean_cat <- house %>% 
              group_by(Garage) %>% 
              summarise(m=mean(Price))
#plot the mean values for the diffrent bedrooms:
ggplot(data=mean_cat, aes(x=Garage,y=m)) +
  geom_point()+
  labs(title="Plot of the mean values of the price against the number of garages")

```
The range of parking spots goes from 0 to 7, with a majority of the houses having 2 parking spots, followed by 3 and 2. There are no missing values and the data lies within {0,...,7}.
There is a slight positive relatinship between the number of garages and the price of a house. However, it is very weak.


"Pool":
```{r}
#generate a frequency table and calculate percentage:
pool_table <-table(Pool)
pool_table
100*pool_table[2]/(pool_table[1]+pool_table[2])

#null values?
sum(is.na(Pool))

#Is a house with a pool worth more?
house %>% 
    group_by(Pool) %>% 
    summarise(m=mean(Price))

```
Almost 7% of the properties have a pool. A house with a pool is worth on average 352120$, wile a house without a pool is worth on average 272396$.



```{r}
#barplot
ggplot(data=house, aes(YearBuild)) +
  geom_bar()+
  labs(title="Barplot of the variable YearBuild")

summary(YearBuild)

#null values?
sum(is.na(YearBuild))

#scatterplot between the year and the price:
ggplot(data=house, aes(x=YearBuild,y=Price))+
  geom_point()+
  labs(title="Scatter plot of the price against the year in which the house was built")

  
```
The build-year for the different houses is spread from 1885 to 1998. The mean year is 1967. The number of houses built increases and reaches a spike around 1960.It then decreases again reaching a low in 1975, before rising sharply again and then slowly decreasing again.
There seems to be a relationship between the year in which the house was built and its price. A new house has a higher price tag than an old one. However, this relationship is not very strong.

"Quality":
```{r}
#table for the quality:
table(Quality)

#mean values for different quality houses:
house %>% 
  group_by(Quality) %>% 
  summarise(mean(Price))
          

#null values?
sum(is.na(Quality))
```
In our data set, 290 houses are built n quality 2, 164 in quality 3 and 68 in quality 1. Following those numbers, it seems that the high quality houses are denoted with 1, medium quality with 2 and low quality/cheap with 3. This seems to be confirmed by the mean of the prices for the different build-qualities. The mean of the prices for build-qulity 1 is 543611$, for build-quality 2 it is 273766, and for built-quality 3, it is 175018. We conclude that the quality has an influence on the housing prices.


"Lot":
```{r}
#histogram:
ggplot(data=house, aes(Lot)) +
  geom_histogram(col="white",
                 bins=35) +
  labs(title="Histogram for Lot")

#is it normal?
qqnorm(Lot)

#boxplot:
ggplot(data=house, aes(x="", y=Lot)) +
  geom_boxplot()+
  labs(title="Boxplot for Lot")

#summary statistics:
summary(Lot)

#null values?
sum(is.na(Lot))

#scatter plot with Price
ggplot(data=house,aes(x=Lot,y=Price))+
  geom_point() +
  labs(title="Scatter plot of Lot(x) against Price(y)")
#are they correlated?
cor(Price,Lot)
```
The median size of the land is 22200, while the mean size is 24370. This is due to the fact that the values in "Lot" are right-skewd. The values last from 4560 to 86830, and there are no null values.
The scatter plot of "Lot" and "Price" shows a slight positive correlation (which is confirmed by the Pearson's correlation which is 0.224). There is a slight tendency that suggests that houses on larger land surfaces are sold for more money than properties on small lands. However, this correlation is not very strong.


AdjHighway:
```{r}
#table for the quality:
table(AdjHighway)

#mean for the different values of the categorical variable:
house %>% 
  group_by(AdjHighway) %>% 
  summarise(mean(Price))
          

#null values?
sum(is.na(AdjHighway))
```
A large maority of the properties in our data set is not next to a highway. Only 11 out of 522 houses are next to one. Looking at the mean prices of houses next to a highway (2300027$) compared to prices of houses that are not next to a highway (230027$), we can assume that there is a relationship between those to types of properties. However, the number of houses next to the highway is small, so it is difficult to say if the price difference is caused by the fact of being close to the highway.



#First model:
A first and very simple approach to predict the housing prices is to make use of the strong linear correlation of the size and the price of a property. As such, we can generate a simple linear regression model and plot its result.
```{r}
simple_reg <- lm(Price ~ Sqft)
summary(simple_reg)

ggplot(house, aes(x=Sqft, y=Price)) +
    geom_point() +   
    geom_smooth(method=lm)+   # Add linear regression line 
    labs(title="Scatter plot of the price vs the size of the house (sqft)")                      
```
This very simple model predicts the housing prices based on the size of the houses. The p-values for Sqft is very small, so the predictor is meaningful for our model. The R2 value is 0.6715, so our model can account for 67,15% of the variability of the data. All in all, even though the model is very simple, it is not that bad. However, as we have a lot more data, we can generate a far better prediction. 

We are going to proceed as follows. First, we are going to generate a model including as many variables as possible. Next we are trying to improve the model by removing dependent variables, and by encoding the categorical variables. Finally, we are going to come up with new variables that are derived from the existing data set. The final model will then be analysed in detail from a statistical point of view.



# Hypothesis study:
In this section we are going to test a few hypothesis, which will help us to improve our model in the feature engineering part. During our data exploration, we have encountered a few relationships that seem to exist between the price and different variables. However, a statistical test is needed to quantify them and to generalize from our data set. Indeed, we only have data on 522 houses and their prices, and we were only able of visualizing this data. A statistical test allows us to have an iea if our assumption is true for the population as well (and not only for the randomly drwan sample).

```{r}

```

# Dummy encoding for categorical data:
Categrorical data can also be included in a linear regression model. However, it needs to be encoded as dummy variables. This is due to the fact that the regression model tresta all independent variables as numerical ones. Numerical variables encode data that can be directly compared. This is not the case for catergical data


To consider:
1) Are the variables in the model independent?
2) Are they categorical or contiuous variables? --> dummy variables : https://www.moresteam.com/whitepapers/download/dummy-variables.pdf 



```{r}
plot(house)
cor(house)
#a lot of linear reationships
```


